---
pagetitle: "ETC3250/5250: Introduction to Machine Learning"
unitcode: "ETC3250/5250"
unitname: "Introduction to Machine Learning"
subtitle: "Neural network I"
author: "Emi Tanaka"
email: "emi.tanaka@monash.edu"
date: "Week 11"
department: "Department of Econometrics and Business Statistics"
unit-url: "iml.numbat.space"
footer: "ETC3250/5250 Week 11"
format: 
  revealjs:
    html-math-method: katex
    logo: images/monash-one-line-black-rgb.png
    slide-number: c/t
    multiplex: false
    theme: assets/monash.scss
    show-slide-number: all
    show-notes: true
    controls: true
    width: 1280
    height: 720
    css: [assets/tachyons-addon.css, assets/custom.css]
    include-after-body: "assets/after-body.html"
    auto-stretch: false
    toc: true
    toc-title: "[*Neural network I*]{.monash-blue} - table of contents"
    chalkboard:
      boardmarker-width: 5
      buttons: true
execute:
  echo: true
---


```{r setup, include = FALSE}
library(tidyverse)
current_file <- knitr::current_input()
basename <- gsub(".[Rq]md$", "", current_file)
theme_set(theme_bw(base_size = 18))
knitr::opts_chunk$set(
  fig.path = sprintf("images/%s/", basename),
  fig.width = 10,
  fig.height = 5,
  fig.align = "center",
  fig.retina = 2,
  echo = TRUE,
  warning = FALSE,
  message = FALSE,
  cache = TRUE,
  cache.path = sprintf("cache/%s/", basename)
)
```

## <br>[`r rmarkdown::metadata$unitcode`]{.monash-blue} {background-image="images/bg-01.png" #etc5523-title}

[**`r rmarkdown::metadata$unitname`**]{.monash-blue .f1}

### `r rmarkdown::metadata$subtitle`

Lecturer: *`r rmarkdown::metadata$author`*

`r rmarkdown::metadata$department`


## Supervised learning 

::: incremental

- So far you have learnt about:
  - linear, non-linear & logistic regression 
  - linear & quadratic discriminant analysis
  - decision trees 
  - tree-ensemble methods (bagging, boosting, random forest)
  - $k$-nearest neighbours 
  - support vector machine methods 
- But these methods still don't perform well for some tasks, e.g. image recognition. 

:::

## <i class='fas fa-brain'></i> Human brain

- Our brains can dissect and process features of images, e.g. the shape, object, lighting, etc. 

```{r fashion-data}
#| echo: false
library(tidyverse)
fashion <- read_csv("https://emitanaka.org/iml/data/fashion-mnist_train.csv.gz")
fashion_test <- read_csv("https://emitanaka.org/iml/data/fashion-mnist_test.csv")
```

```{r display1}
#| echo: false
#| fig-height: 2
#| fig-width: 12
set.seed(2)
fashion_test %>% 
  mutate(id = 1:n()) %>% 
  slice(sample(id, 10)) %>% 
  pivot_longer(-c(label, id)) %>% 
  group_by(id) %>% 
  mutate(row = rep(1:28, each = 28),
         col = rep(1:28, times = 28)) %>% 
  ggplot(aes(col, row)) +
  geom_tile(aes(fill = value)) + 
  scale_y_reverse() + 
  facet_wrap(~id, nrow = 1) + 
  coord_equal() +
  scale_fill_distiller(palette = 1, direction = 1) +
  theme(axis.text = element_blank(),
        axis.title = element_blank(),
        axis.ticks.length = unit(0, "mm"),
        strip.background = element_blank(),
        strip.text = element_blank()) +
  guides(fill = "none")
```

. . . 

- The human brain is made of billions of neurons that communicate via electrochemical signals. 


. . . 

- So how do we mimic this in a program?






# Artificial neuron {background-color="#006DAE" .mcenter}

## Biological neuron model

::: flex

::: {.w-60 }

::: incremental

- Artificial neural network, or often referred to just as [**neural network**]{.monash-blue}, was inspired by the _biological_ neural network.
- In a _biological_ neural network, a collection of neurons interconnected by _[synapses]{.monash-blue} carry out a specific function when activated_. 


:::

:::

::: {.w-40 .pl3}

<center>
![](images/Blausen_0657_MultipolarNeuron.png){width="100%"}
</center>


:::

:::

::: fragment

- The [*dendrites*]{.monash-blue} receive synaptic inputs and  propogate electrochemical stimulation to the cell body - if stimulated enough, a neuron fires an [_action potential_]{.monash-blue} (synaptic inputs for other neurons).

:::


::: aside 


Image from [wikipedia](https://en.wikipedia.org/wiki/Neural_circuit#/media/File:Blausen_0657_MultipolarNeuron.png).

:::






## Artificial neuron: dendrites



::: flex

::: {.w-60}

- The **artificial neuron** is the elementary units of an artificial neural network.

::: fragment

- The artificial neuron receives predictors $\boldsymbol{x}_i = (1, x_{i1}, \dots,x_{ip})^\top$ that is typically combined as a weighted sum: 

:::

:::

::: {.w-40 .pl3}

<center>
![](images/Neuron3.svg){width="100%"}
</center>


:::

:::

::: fragment

$$z_i = \beta_0 + \sum_{j=1}^p\beta_jx_{ij} = \boldsymbol{\beta}^\top\boldsymbol{x}_i, \quad\text{where }\boldsymbol{\beta} = (\beta_0, \beta_1, \dots, \beta_p)^\top.$$

:::

::: aside 

Image from [wikipedia](https://en.wikipedia.org/wiki/File:Neuron3.svg).

:::

## Artificial neuron: action potential


::: incremental

- The $z_i$ then get passed into the [**activation function**]{.monash-blue}, $h(z_i)$.
- Suppose $a$ is the location parameter and $w$ is the scale parameter.
- Some common choices include:
  - **Heaviside step**: $h(z_i) = \mathbb{I}(z_i > 0)$,
  - **Linear**: $h(z_i) = z_i$,
  - **Sigmoid**: $h(z_i|a,w) = a + w(1+e^{-z_i})^{-1}$,
  - **Tanh**: $h(z_i|a,w) = a + w \left(\frac{2}{1+e^{-2z_i}} - 1 \right)$, and
  - **ReLU**: $h(z_i|a,w) = a + w \times \max(0, z_i)$.

:::


## Visualising artifical neuron

::: flex

::: {.w-50}

```{r vis-AN}
#| echo: false
library(DiagrammeR)
grViz("
digraph gr1 {
  rankdir=LR

  node [shape = circle]

  subgraph cluster0 {
    style=filled;
    color=lightgrey;
    A1 [label='1']; 
    A2 [label='x\u2081']; 
    A3 [label='x\u2082']; 
    label='Input layer';
  }

  subgraph cluster1 {
    label='Output layer';
    style=filled;
    color=lightyellow;
    B [label='ReLU\na = 1, w = 2']; 
  }

  # several 'edge' statements
  A1->B [label='\u03B2\u2080=1']
  A2->B [label='\u03B2\u2081=0.5']
  A3->B [label='\u03B2\u2082=-3']

}
", width = 600)
```


:::

::: {.w-50 .pl3}

::: incremental

- When $x_1 = 1$ and $x_2 = 3$, then $$\begin{align*}z &= \beta_0 + \beta_1x_1 + \beta_2 x_2\\ &= 1 + 0.5 \times 1 - 3\times 3 = -7.5.\end{align*}$$

- Using ReLU with $a = 1, w = 2$, the prediction is $$1 + 2 \times \max(0, z) = 1.$$


:::

:::

:::


## Predicting from an artificial neuron

::: flex

::: {.w-50}

```{r vis-AN2}
#| echo: false
m <- grViz("
digraph gr2 {
  rankdir=LR

  node [shape = circle]

  A1 [label='1']; 
  A2 [label='x\u2081']; 
  B [label='ReLU\na = 3, w = -0.5']; 
  A1->B [label='\u03B2\u2080=3']
  A2->B [label='\u03B2\u2081=-5']

}
", width = 600)
widgetframe::frameWidget(m)
```


:::

::: {.w-50 .pl3}

*Your turn!*

- What is the prediction when $x_1 = 10$? 

[$$3$$]{.fragment}


- What is the prediction when $x_1 = -5$?

[$$-11$$]{.fragment}


:::

:::


# Activation function {background-color="#006DAE" .mcenter}

## Heaviside step 

$$h(z_i) = \mathbb{I}(z_i > 0)$$

* This is also known as the **perceptron** and is used for classification.

. . . 

* For example: [$h(1 + 2x_i)$]{.monash-red2} and [$h(-12 + 4x_i)$]{.monash-blue2}.

```{r heaviside}
#| echo: false
#| fig-height: 3.5
gbase <- tibble(x = seq(-4, 4, by = 0.01),
       y1 =  as.numeric((1 + 2 * x) > 0),
       y2 =  as.numeric((-12 + 4 * x) > 0)) %>% 
   ggplot(aes(x)) + 
   geom_line(aes(y = y1), color='#EE0220', size=1.3)+ 
   geom_line(aes(y = y2), color='#027EB6', size=1.3, linetype = 'dashed') +
  labs(x = "x", y = "h(x)")

gbase
```

## Linear 

$$h(z_i) = z_i$$

- This is a regression model!
- For example: [$h(1 + 2x_i)$]{.monash-red2} and [$h(-12 + 4x_i)$]{.monash-blue2}.

```{r linear-fn}
#| echo: false
#| fig-height: 3.5
gbase %+% tibble(x = seq(-4, 4, 0.1),
                 y1 = 1 + 2 * x,
                 y2 = -12 + 4 * x) 
```



## Sigmoid (Logistic)

$$h(z_i|a,w) = a + w(1+e^{-z_i})^{-1}$$


- When $a = 0$ and $w = 1$, then $0 < h(z_i) < 1$ for finite $z_i$.
* E.g. [$h(1 + 2x_i|a = 0, w = 1)$]{.monash-red2} and [$h(-12 + 4x_i|a = -1, w = 2)$]{.monash-blue2}.

```{r sigmoid}
#| echo: false
#| fig-height: 3.5
gbase %+% tibble(x = seq(-10, 10, 0.1),
                 y1 = 1/(1 + exp(-1 - 2 * x)),
                 y2 = -1 + 2/(1 + exp(12 - 4 * x)))
```


## Hyperbolic tangent (Tanh)

$$h(z_i) = a + w \left(\frac{2}{1+e^{-2z_i}} - 1 \right)$$

- Similar to Sigmoid.
* E.g. [$h(1 + 2x_i|a = 0, w = 1)$]{.monash-red2} and [$h(-12 + 4x_i|a = -1, w = 2)$]{.monash-blue2}.


```{r tanh}
#| echo: false
#| fig-height: 3.5
gbase %+% tibble(x = seq(-10, 10, 0.1),
                 y1 = 2/(1 + exp(-1 - 2 * x)) - 1,
                 y2 = -1 + 2 * (1/(1 + exp(12 - 4 * x)) - 1))
```

## Rectified linear unit (ReLU)

$$h(z_i) = a + w \times \max(0, z_i)$$

- Sigmoid and Tanh are largely replaced by **rectified linear unit** (ReLU).
* E.g. [$h(1 + 2x_i|a = 0, w = 1)$]{.monash-red2} and [$h(-12 + 4x_i|a = -1, w = 2)$]{.monash-blue2}.

```{r relu}
#| echo: false
#| fig-height: 3.5
gbase %+% (tibble(x = seq(-10, 10, 0.1)) %>% 
  rowwise() %>% 
  mutate(y1 = max(0, x),
         y2 = -1 + 2 * max(0, x)))
```



# Neural network {background-color="#006DAE" .mcenter}

## Limitations of a single artificial neuron

::: incremental


- Biological neurons are interconnected in complex networks that allow the brain to perform a wide range of functions. 
- A single artificial neuron has a limited ability to model complex relationships so much like its biological counterpart, we can connect different artificial neurons to model complex relationships.
- An artifical **neural network**, the interconnection of the artificial neurons, began by mimicing the architecture of brain activity. 
- But neural network is no longer true to their biological counterpart and its development is motivated by empirical results. 

:::

## Multiple artificial neurons

- When combining artificial neurons, we always set $a = 0$ and $w = 1$ for Sigmoid, Tanh, and ReLU.

::: flex

::: {.w-50}

<center>
[Artificial neuron 1]{.monash-blue2}
</center>


```{r vis-AN3a}
#| echo: false
m1 <- grViz("
digraph gr3a {
  rankdir=LR
  node [shape = circle]
  A1 [label='1']; 
  A2 [label='x\u2081']; 
  B [label='ReLU\na = 0, w = 1', color='#027EB6', style='filled',fontcolor=white]; 
  A1->B [label='\u03B2\u2080=3', color='#027EB6', fontcolor='#027EB6']
  A2->B [label='\u03B2\u2081=-5', color='#027EB6', fontcolor='#027EB6']

}
", width = 400)
widgetframe::frameWidget(m1, elementId = "gr3a")
```

:::

::: {.w-50 .pl3}

<center>
[Artificial neuron 2]{.monash-red2}
</center>

```{r vis-AN3b}
#| echo: false
m2 <- grViz("
digraph gr3b {
  rankdir=LR
  node [shape = circle]
  A1 [label='1']; 
  A2 [label='x\u2081']; 
  B [label='ReLU\na = 0, w = 1', color='#EE0220', style='filled',fontcolor=white]; 
  A1->B [label='\u03B2\u2080=1', color='#EE0220', fontcolor='#EE0220']
  A2->B [label='\u03B2\u2081=0.5', color='#EE0220', fontcolor='#EE0220']
}
", width = 400)
widgetframe::frameWidget(m2, elementId = "gr3b")
```

:::

:::


## Combining artificial neurons for regression

- In general, we combine $K$ neurons as $$f(\boldsymbol{x}_i) = b + \sum_{k=1}^Kw_kh(\boldsymbol{\beta}_k^\top\boldsymbol{x}_i)$$
  - $\boldsymbol{\beta}_k$ is the coefficient of predictors in the $k$-th artificial neuron,
  - $b$ is called the **bias**,
  - $w_k$ is the **weight** corresponding to the $k$-th neuron.

- The activation function $h$ is always of the same type.

## Example of combining artificial neurons

$$f(\boldsymbol{x}_i) = b + w_1{\color{#027EB6}{h(\boldsymbol{\beta}_1^\top\boldsymbol{x}_i)}} + w_2\color{#EE0220}{h(\boldsymbol{\beta}_2^\top\boldsymbol{x}_i)}$$

::: flex

::: {.w-70}

```{r vis-ANcombined}
#| echo: false
mcombine <- grViz("
digraph grcombined {
  rankdir=LR
  splines=false
  node [shape = circle]
  A1 [label='1']; 
  A2 [label='x\u2081']; 
  B1 [label='ReLU', color='#027EB6', style='filled',fontcolor=white]; 
  B2 [label='ReLU', color='#EE0220', style='filled',fontcolor=white]; 
  C [label = 'Linear\nb=0'];
  A1->B1 [label='3', color='#027EB6', fontcolor='#027EB6']
  A2->B1 [label='-5', color='#027EB6', fontcolor='#027EB6']
  A1->B2 [label='1', color='#EE0220', fontcolor='#EE0220']
  A2->B2 [label='0.5', color='#EE0220', fontcolor='#EE0220']
  B1->C [label='0.5']
  B2->C [label='0.9']
}
}
", width = 800, height = 500)
widgetframe::frameWidget(mcombine, elementId = "grcombine", height = 500)
```


:::

::: {.w-30 .pl3 .f4}

This represents a neural network with 

  - 2 nodes in the input layer, 
  - 2 nodes in the middle layer, and 
  - 1 node in the output layer with parameters:
    - $b = 0$, 
    - $w_1 = 0.5$, 
    - $w_2 = 0.9$, 
    - $\boldsymbol{\beta}_1 = (3, -5)^\top$,
    - $\boldsymbol{\beta}_2 = (1, 0.5)^\top$, and 
    - $h$ is the ReLU activation function with $a = 0$ and $w = 1$.



:::

:::

## Combining artificial neurons for classification 

::: incremental

- The output of the previous example is only applicable for regression problems.
- We can easily modify this for classfication by changing the output layers to say the Sigmoid function, which gives a numerical value between 0 and 1 and can be thought of as the propensity score. $$P(y_i = 1 | \boldsymbol{x}_i) = \frac{1}{1 + \exp\left(-(b + \sum_{k=1}^Kw_kh(\boldsymbol{\beta}_k^\top\boldsymbol{x}_i))\right)}.$$

:::

## Regression vs classification 

- In a neural network, initial layers can be identical for regression or classification.

. . . 

- It is the output layer that determines if it can be used for [regression]{.monash-orange2} or [classification]{.monash-green2}!


::: flex

::: {.w-50}

```{r vis-ANcombined2}
#| echo: false
mcombine <- grViz("
digraph grcombined2 {
  rankdir=LR
  splines=false
  node [shape = circle]
  A1 [label='1']; 
  A2 [label='x\u2081']; 
  B1 [label='ReLU', color='#027EB6', style='filled',fontcolor=white]; 
  B2 [label='ReLU', color='#EE0220', style='filled',fontcolor=white]; 
  C [label = 'Linear\nb=0', fontcolor='#D93F00'];
  A1->B1 [label='3', color='#027EB6', fontcolor='#027EB6']
  A2->B1 [label='-5', color='#027EB6', fontcolor='#027EB6']
  A1->B2 [label='1', color='#EE0220', fontcolor='#EE0220']
  A2->B2 [label='0.5', color='#EE0220', fontcolor='#EE0220']
  B1->C [label='0.5']
  B2->C [label='0.9']
}
}
", width = 800, height = 400)
widgetframe::frameWidget(mcombine, elementId = "grcombine2", height = 400)
```


:::

::: {.w-50 .pl3}

```{r vis-ANcombined3}
#| echo: false
mcombine <- grViz("
digraph grcombined3 {
  rankdir=LR
  splines=false
  node [shape = circle]
  A1 [label='1']; 
  A2 [label='x\u2081']; 
  B1 [label='ReLU', color='#027EB6', style='filled',fontcolor=white]; 
  B2 [label='ReLU', color='#EE0220', style='filled',fontcolor=white]; 
  C [label = 'Sigmoid\nb=0', fontcolor='#008A25'];
  A1->B1 [label='3', color='#027EB6', fontcolor='#027EB6']
  A2->B1 [label='-5', color='#027EB6', fontcolor='#027EB6']
  A1->B2 [label='1', color='#EE0220', fontcolor='#EE0220']
  A2->B2 [label='0.5', color='#EE0220', fontcolor='#EE0220']
  B1->C [label='0.5']
  B2->C [label='0.9']
}
}
", width = 800, height = 400)
widgetframe::frameWidget(mcombine, elementId = "grcombine3", height = 400)
```


:::

:::

# Multi-class classification {background-color="#006DAE" .mcenter}

## Multi-class classification 

::: incremental

- The Sigmoid functions allows for computation of propensity scores for binary outcomes.
- If you have more than two classes in your response, then you need to convert it to dummy variables, or otherwise referred to as **_one-hot encoding_**.
- So for a categorical variable with $m$ levels, $y_i \in \{\text{Class 1}, \dots, \text{Class }m\}$, we convert it as: $$y_{ik} = \begin{cases}1 & \text{if } y_i = \text{Class}k\\0 & \text{if } y_i \neq \text{Class}k\end{cases}$$

:::


## <i class='fab fa-r-project'></i>  From categorical variable to dummy variables

```{r}
dat <- tibble(pet = c("cat", "dog", "cat", "fish"))
model.matrix(~ pet - 1, data = dat)
```

. . . 

Alternatively, 

```{r}
class_levels <- as.numeric(factor(dat$pet)) - 1 # changes to cat = 0, dog = 1, fish = 2 
keras::to_categorical(class_levels, num_classes = 3)
```

## Softmax activation function 

::: incremental

- The Sigmoid function only works for $m = 2$.
- For $m > 2$, we can use the _Softmax_ activation function instead: $$P(y_{ij} = 1 | \boldsymbol{x}_i) = \frac{\exp(\boldsymbol{\beta}_j^\top\boldsymbol{x}_i)}{\sum_{j=1}^m\exp\left(\boldsymbol{\beta}_j^\top\boldsymbol{x}_i\right)}.$$

- The number of neurons for the Softmax layer must be $m$. 
- Note that $\sum_{j=1}^m P(y_{ij} = 1 | \boldsymbol{x}_i) = 1$.

:::


## An illustration of a Softmax layer

::: flex

::: {.w-50}

::: incremental

- Suppose we have the income of a customer in thousands of dollar. 
- We want to predict if the customer will buy a "cheap", "average" or "expensive" brand of clothing brand.
- *What is the probability that customer with an income of $45K will buy a cheap brand based on this trained neural network?*

:::


:::

::: {.w-50 .pl3}


```{r vis-softmax1}
#| echo: false
mcombine <- grViz("
digraph softmax1 {
  rankdir=LR
  splines=false
  node [shape = circle, width=1.3,height=1.3,fixedsize=true]
  A1 [label='1']; 
  A2 [label='Income']; 
  B1 [label='ReLU', color='#027EB6', style='filled',fontcolor=white]; 
  B2 [label='ReLU', color='#EE0220', style='filled',fontcolor=white]; 
  B3 [label='ReLU', color='#008A25', style='filled',fontcolor=white]; 
  C1 [label = 'Softmax\n(0 = cheap)'];
  C2 [label = 'Softmax\n(1 = average)'];
  C3 [label = 'Softmax\n(2 = expensive)'];
  A1->B1 [label='49.62', color='#027EB6', fontcolor='#027EB6']
  A2->B1 [label='-0.37', color='#027EB6', fontcolor='#027EB6']
  A1->B2 [label='27.62', color='#EE0220', fontcolor='#EE0220']
  A2->B2 [label='-0.19', color='#EE0220', fontcolor='#EE0220']
  A1->B3 [label='-1.72', color='#008A25', fontcolor='#008A25']
  A2->B3 [label='0.35', color='#008A25', fontcolor='#008A25']
  B1->C1 [color='#027EB6', fontcolor='#027EB6', label = '1']
  B1->C2 [color='#027EB6', fontcolor='#027EB6', label = '0']
  B1->C3 [color='#027EB6', fontcolor='#027EB6', label = '0']
  B2->C1 [color='#EE0220', fontcolor='#EE0220', label = '0']
  B2->C2 [color='#EE0220', fontcolor='#EE0220', label = '1']
  B2->C3 [color='#EE0220', fontcolor='#EE0220', label = '0']
  B3->C1 [color='#008A25', fontcolor='#008A25', label = '0']
  B3->C2 [color='#008A25', fontcolor='#008A25', label = '0']
  B3->C3 [color='#008A25', fontcolor='#008A25', label = '1']
  

}
}
", width = 800, height = 500)
widgetframe::frameWidget(mcombine, elementId = "softmax1", height = 500)
```


:::

:::


## Solution 

::: flex

::: {.w-50}


```{r vis-softmax2}
#| echo: false
mcombine <- grViz("
digraph softmax2 {
  rankdir=LR
  splines=false
  node [shape = circle, width=1.3,height=1.3,fixedsize=true]
  A1 [label='1']; 
  A2 [label='Income']; 
  B1 [label='ReLU', color='#027EB6', style='filled',fontcolor=white]; 
  B2 [label='ReLU', color='#EE0220', style='filled',fontcolor=white]; 
  B3 [label='ReLU', color='#008A25', style='filled',fontcolor=white]; 
  C1 [label = 'Softmax\n(0 = cheap)'];
  C2 [label = 'Softmax\n(1 = average)'];
  C3 [label = 'Softmax\n(2 = expensive)'];
  A1->B1 [label='49.62', color='#027EB6', fontcolor='#027EB6']
  A2->B1 [label='-0.37', color='#027EB6', fontcolor='#027EB6']
  A1->B2 [label='27.62', color='#EE0220', fontcolor='#EE0220']
  A2->B2 [label='-0.19', color='#EE0220', fontcolor='#EE0220']
  A1->B3 [label='-1.72', color='#008A25', fontcolor='#008A25']
  A2->B3 [label='0.35', color='#008A25', fontcolor='#008A25']
  B1->C1 [color='#027EB6', fontcolor='#027EB6', label = '1']
  B1->C2 [color='#027EB6', fontcolor='#027EB6', label = '0']
  B1->C3 [color='#027EB6', fontcolor='#027EB6', label = '0']
  B2->C1 [color='#EE0220', fontcolor='#EE0220', label = '0']
  B2->C2 [color='#EE0220', fontcolor='#EE0220', label = '1']
  B2->C3 [color='#EE0220', fontcolor='#EE0220', label = '0']
  B3->C1 [color='#008A25', fontcolor='#008A25', label = '0']
  B3->C2 [color='#008A25', fontcolor='#008A25', label = '0']
  B3->C3 [color='#008A25', fontcolor='#008A25', label = '1']
  

}
}
", width = 800, height = 500)
widgetframe::frameWidget(mcombine, elementId = "softmax2", height = 500)
```


:::

::: {.w-50 .pl3 .f4}

::: incremental

**Layer 2**

- [ReLU]{.monash-blue2}: $\max(0, 49.62 - 0.37 \times 45) = `r max(0, 49.62 - 0.37 * 45)`$
- [ReLU]{.monash-red2}: $\max(0, 27.62 - 0.19 \times 45) = `r max(0, 27.62 - 0.19 * 45)`$
- [ReLU]{.monash-green2}: $\max(0, -1.72 + 0.35 \times 45) = `r max(0, -1.72 + 0.35 * 45)`$

**Output layer**

- [Cheap]{.monash-blue2}: $$\frac{\exp(32.97)}{\exp(32.97) + \exp(19.07) + \exp(14.03)} = `r exp(32.97)/(exp(32.97) + exp(19.07) + exp(14.03))`$$
- [Average]{.monash-red2}: $$\frac{\exp(19.07)}{\exp(32.97) + \exp(19.07) + \exp(14.03)} = `r scales::comma(exp(19.07)/(exp(32.97) + exp(19.07) + exp(14.03)), 0.00000001)`$$ 
- [Expensive]{.monash-green2}: $$\frac{\exp(14.03)}{\exp(32.97) + \exp(19.07) + \exp(14.03)} = `r scales::comma(exp(14.03)/(exp(32.97) + exp(19.07) + exp(14.03)), 0.0000000001)`$$

:::

[**Prediction**: The customer will buy the cheap brand.]{.fragment}

:::

:::



# Building a neural network structure with R {background-color="#006DAE" .mcenter}

## <i class='fab fa-python'></i> Installing keras

::: incremental

- **Keras** is an open-source software library that uses the **TensorFlow** library to fit artificial neural networks.
- We can use the Keras library through the `keras` package in R. 
- To install `keras`, run the following commands:

:::

. . . 

```r
install.packages("keras")
library(keras)
install_keras(method = c("conda"), 
              conda = "auto", 
              tensorflow = "default",
              extra_packages = "tensorflow-hub")
```

. . . 

- <i class='fas fa-exclamation-circle'></i> Be warned that the installation poses issues often due to `keras` looking at the wrong location for the Keras library.


## <i class='fab fa-r-project'></i> Building a neural network structure in R

::: flex

::: {.w-50}

```{r vis-softmax3}
#| echo: false
mcombine <- grViz("
digraph softmax3 {
  rankdir=LR
  splines=false
  node [shape = circle, width=1.3,height=1.3,fixedsize=true]
  A1 [label='1']; 
  A2 [label='Income']; 
  B1 [label='ReLU', color='#027EB6', style='filled',fontcolor=white]; 
  B2 [label='ReLU', color='#EE0220', style='filled',fontcolor=white]; 
  B3 [label='ReLU', color='#008A25', style='filled',fontcolor=white]; 
  C1 [label = 'Softmax\n(0 = cheap)'];
  C2 [label = 'Softmax\n(1 = average)'];
  C3 [label = 'Softmax\n(2 = expensive)'];
  A1->B1 [ fontcolor='#027EB6']
  A2->B1 [ fontcolor='#027EB6']
  A1->B2 [ fontcolor='#EE0220']
  A2->B2 [ fontcolor='#EE0220']
  A1->B3 [ fontcolor='#008A25']
  A2->B3 [ fontcolor='#008A25']
  B1->C1 [color='#027EB6']
  B1->C2 [color='#027EB6']
  B1->C3 [color='#027EB6']
  B2->C1 [color='#EE0220']
  B2->C2 [color='#EE0220']
  B2->C3 [color='#EE0220']
  B3->C1 [color='#008A25']
  B3->C2 [color='#008A25']
  B3->C3 [color='#008A25']
  

}
}
", width = 800, height = 500)
widgetframe::frameWidget(mcombine, elementId = "softmax3", height = 500)
```


:::

::: {.w-50 .pl3}


```{r keras}
library(keras)
model <- keras_model_sequential() %>%
  layer_dense(units = 3,
              input_shape = 2,
              activation = "relu") %>% 
  layer_dense(units = 3, 
              activation = "softmax")

```

::: f3

- `keras_model_sequential()` must be used first to initialise the architechture.
- `layer_dense()` indicates a new layer with:
  - `units` indicating the number of neurons in that layer,
  - `input_shape` indicating the number of predictors (only needed for the first `layer_dense()`),
  - `activation` specifying the activation function for that layer.

:::

:::

:::

## <i class='fab fa-r-project'></i> Examining the weights and biases

- You can extract the weights and biases using `get_weights()`:

::: flex

::: {.w-50}

```{r get-weights}
#| echo: 3
#| cache: false
library(keras)
model <- keras_model_sequential() %>%
  layer_dense(units = 3,
              input_shape = 2,
              activation = "relu") %>% 
  layer_dense(units = 3, 
              activation = "softmax")
get_weights(model)
```


:::

::: {.w-50 .pl3}

- Every even entry is the bias of the nodes (here all 0s).
- The weights are given in every odd entry in the order of the layers.

:::

:::

## <i class='fab fa-r-project'></i> Manually setting the weights

::: flex

::: {.w-50}

```{r set-weights}
#| cache: false
w <- get_weights(model)
w[[1]] <- matrix(c(49.62, -0.37, 27.62, -0.19, -1.72, 0.35), nrow = 2)
w[[3]] <- diag(3)
set_weights(model, w)
get_weights(model)
```

:::

::: {.w-50 .pl3}

- Let's manually set the weights as the previous example:

```{r vis-softmax4}
#| echo: false
mcombine <- grViz("
digraph softmax4 {
  rankdir=LR
  splines=false
  node [shape = circle, width=1.3,height=1.3,fixedsize=true]
  A1 [label='1']; 
  A2 [label='Income']; 
  B1 [label='ReLU', color='#027EB6', style='filled',fontcolor=white]; 
  B2 [label='ReLU', color='#EE0220', style='filled',fontcolor=white]; 
  B3 [label='ReLU', color='#008A25', style='filled',fontcolor=white]; 
  C1 [label = 'Softmax\n(0 = cheap)'];
  C2 [label = 'Softmax\n(1 = average)'];
  C3 [label = 'Softmax\n(2 = expensive)'];
  A1->B1 [label='49.62', color='#027EB6', fontcolor='#027EB6']
  A2->B1 [label='-0.37', color='#027EB6', fontcolor='#027EB6']
  A1->B2 [label='27.62', color='#EE0220', fontcolor='#EE0220']
  A2->B2 [label='-0.19', color='#EE0220', fontcolor='#EE0220']
  A1->B3 [label='-1.72', color='#008A25', fontcolor='#008A25']
  A2->B3 [label='0.35', color='#008A25', fontcolor='#008A25']
  B1->C1 [color='#027EB6', fontcolor='#027EB6', label = '1']
  B1->C2 [color='#027EB6', fontcolor='#027EB6', label = '0']
  B1->C3 [color='#027EB6', fontcolor='#027EB6', label = '0']
  B2->C1 [color='#EE0220', fontcolor='#EE0220', label = '0']
  B2->C2 [color='#EE0220', fontcolor='#EE0220', label = '1']
  B2->C3 [color='#EE0220', fontcolor='#EE0220', label = '0']
  B3->C1 [color='#008A25', fontcolor='#008A25', label = '0']
  B3->C2 [color='#008A25', fontcolor='#008A25', label = '0']
  B3->C3 [color='#008A25', fontcolor='#008A25', label = '1']
  

}
}
", width = 800, height = 500)
widgetframe::frameWidget(mcombine, elementId = "softmax4", height = 500)
```

:::

:::


## <i class='fab fa-r-project'></i> Prediction from neural network model

::: incremental 

- Normally we need to train the model but this will be covered next week.
- Suppose the manually set weights are a result from a trained model. 

:::


::: fragment

- You can predict the probability a customer with an income of $45K will buy a cheap, average or expensive brand with:

```{r predict}
predict(model, cbind(1, 45))
```

:::

. . . 

- Note that you need to have the new data in a matrix format with the intercept!



# <i class='fas fa-key'></i> Takeaways {background-color="#006DAE" .mcenter}


- Neural networks are flexible models that can be used for both regression and classification problems. 
- The activation function in the output layer determine if the neural network can be used for regression or classification.
- More neural network to come next week!
